{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This movie makes me want to throw up every tim...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Listening to the director's commentary confirm...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One of the best Tarzan films is also one of it...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Valentine is now one of my favorite slasher fi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No mention if Ann Rivers Siddons adapted the m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Several years ago the Navy kept a studied dist...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>This is a masterpiece footage in B/W 35mm film...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Such a long awaited movie.. But it has disappo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>When two writers make a screenplay of a horror...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Make no mistake, Maureen O'Sullivan is easily ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  This movie makes me want to throw up every tim...      0\n",
       "1  Listening to the director's commentary confirm...      0\n",
       "2  One of the best Tarzan films is also one of it...      1\n",
       "3  Valentine is now one of my favorite slasher fi...      1\n",
       "4  No mention if Ann Rivers Siddons adapted the m...      0\n",
       "5  Several years ago the Navy kept a studied dist...      1\n",
       "6  This is a masterpiece footage in B/W 35mm film...      1\n",
       "7  Such a long awaited movie.. But it has disappo...      0\n",
       "8  When two writers make a screenplay of a horror...      1\n",
       "9  Make no mistake, Maureen O'Sullivan is easily ...      1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('../data/imdb_train.csv', header=0)\n",
    "\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'label'], dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25000 entries, 0 to 24999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    25000 non-null  object\n",
      " 1   label   25000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 390.8+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     0\n",
       "label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence Of Steps\n",
    "\n",
    "<ul>\n",
    "    <li>Preprocess the strings</li>\n",
    "    <li>split the data</li>\n",
    "    <li>create the data pipelines</li>\n",
    "    <li>train the model</li>\n",
    "    <li>get the metrics</li>\n",
    "    <li>finally compare the models</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Train a Classifier\n",
    "You can work with these models:\n",
    "<ul>\n",
    "<li>LogisticRegression from here: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html</li>\n",
    "<li>DecisionTreeClassifier from here: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html</li>\n",
    "<li>RandomForestClassifier from here: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html</li>\n",
    "<li>GridSearchCV from here: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html</li>\n",
    "<li>RandomizedSearchCV from here: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    12500\n",
       "0    12500\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ravikiran.bhonagiri\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]     ...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ravikiran.bhonagiri\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]     ...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install the library and functions\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Let's get a list of stop words from the NLTK library\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# These words are important for our problem. We don't want to remove them.\n",
    "excluding = ['against', 'not', 'don', \"don't\",'ain', 'aren', \"aren't\", 'couldn', \"couldn't\",\n",
    "             'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", \n",
    "             'haven', \"haven't\", 'isn', \"isn't\", 'mightn', \"mightn't\", 'mustn', \"mustn't\",\n",
    "             'needn', \"needn't\",'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \n",
    "             \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "\n",
    "# New stop word list\n",
    "stop_words = [word for word in stop if word not in excluding]\n",
    "\n",
    "snow = SnowballStemmer('english')\n",
    "\n",
    "def process_text(texts): \n",
    "    final_text_list=[]\n",
    "    for sent in texts:\n",
    "        \n",
    "        # Check if the sentence is a missing value\n",
    "        if isinstance(sent, str) == False:\n",
    "            sent = \"\"\n",
    "            \n",
    "        filtered_sentence=[]\n",
    "        \n",
    "        sent = sent.lower() # Lowercase \n",
    "        sent = sent.strip() # Remove leading/trailing whitespace\n",
    "        sent = re.sub('\\s+', ' ', sent) # Remove extra space and tabs\n",
    "        sent = re.compile('<.*?>').sub('', sent) # Remove HTML tags/markups:\n",
    "        \n",
    "        for w in word_tokenize(sent):\n",
    "            # We are applying some custom filtering here, feel free to try different things\n",
    "            # Check if it is not numeric and its length>2 and not in stop words\n",
    "            if(not w.isnumeric()) and (len(w)>2) and (w not in stop_words):  \n",
    "                # Stem and add to filtered list\n",
    "                filtered_sentence.append(snow.stem(w))\n",
    "        final_string = \" \".join(filtered_sentence) #final string of cleaned words\n",
    " \n",
    "        final_text_list.append(final_string)\n",
    "        \n",
    "    return final_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_df[[\"text\"]],\n",
    "                                                  train_df[\"label\"],\n",
    "                                                  test_size=0.10,\n",
    "                                                  shuffle=True,\n",
    "                                                  random_state=360\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text'], dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the text fields\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing the text fields\")\n",
    "X_train[\"text\"] = process_text(X_train[\"text\"].tolist())\n",
    "X_val[\"text\"] = process_text(X_val[\"text\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Grab model features/inputs and target/output\n",
    "numerical_features = []\n",
    "\n",
    "text_features = ['text']\n",
    "\n",
    "model_features = numerical_features + text_features\n",
    "model_target = 'label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "### COLUMN_TRANSFORMER ###\n",
    "##########################\n",
    "\n",
    "'''\n",
    "\n",
    "# Preprocess the numerical features\n",
    "numerical_processor = Pipeline([\n",
    "    ('num_scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "# Preprocess 1st text feature\n",
    "text_processor_0 = Pipeline([\n",
    "    ('text_vect_0', CountVectorizer(binary=True, max_features=50))\n",
    "])\n",
    "\n",
    "# Preprocess 2nd text feature (larger vocabulary)\n",
    "text_precessor_1 = Pipeline([\n",
    "    ('text_vect_1', CountVectorizer(binary=True, max_features=150))\n",
    "])\n",
    "\n",
    "# Combine all data preprocessors from above (add more, if you choose to define more!)\n",
    "# For each processor/step specify: a name, the actual process, and finally the features to be processed\n",
    "data_preprocessor = ColumnTransformer([\n",
    "    ('numerical_pre', numerical_processor, numerical_features),\n",
    "    ('text_pre_0', text_processor_0, text_features[0]),\n",
    "    ('text_pre_1', text_precessor_1, text_features[1])\n",
    "]) \n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "text_processor_label = Pipeline([\n",
    "    ('text_vect', CountVectorizer(binary=True, max_features=50))\n",
    "])\n",
    "\n",
    "data_preprocessor = ColumnTransformer([\n",
    "    ('text_pre_l', text_processor_label, text_features[0])\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[876 394]\n",
      " [344 886]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.69      0.70      1270\n",
      "           1       0.69      0.72      0.71      1230\n",
      "\n",
      "    accuracy                           0.70      2500\n",
      "   macro avg       0.71      0.71      0.70      2500\n",
      "weighted avg       0.71      0.70      0.70      2500\n",
      "\n",
      "Accuracy (validation): 0.7048\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('data_preprocessing', data_preprocessor),\n",
    "    ('logistic_regression', LogisticRegression(penalty = 'l2',\n",
    "                              C = 0.1))\n",
    "                    ])\n",
    "\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "pipeline\n",
    "\n",
    "pipeline.fit(X_train, y_train.values)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "# Use the fitted pipeline to make predictions on the validation dataset\n",
    "val_predictions = pipeline.predict(X_val)\n",
    "print(confusion_matrix(y_val.values, val_predictions))\n",
    "print(classification_report(y_val.values, val_predictions))\n",
    "print(\"Accuracy (validation):\", accuracy_score(y_val.values, val_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[777 493]\n",
      " [339 891]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.61      0.65      1270\n",
      "           1       0.64      0.72      0.68      1230\n",
      "\n",
      "    accuracy                           0.67      2500\n",
      "   macro avg       0.67      0.67      0.67      2500\n",
      "weighted avg       0.67      0.67      0.67      2500\n",
      "\n",
      "Accuracy (validation): 0.6672\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('data_preprocessing', data_preprocessor),\n",
    "    ('decision_tree_classifier', DecisionTreeClassifier(max_depth = 10,\n",
    "                                             min_samples_leaf = 15))\n",
    "                    ])\n",
    "\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "pipeline\n",
    "\n",
    "pipeline.fit(X_train, y_train.values)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "# Use the fitted pipeline to make predictions on the validation dataset\n",
    "val_predictions = pipeline.predict(X_val)\n",
    "print(confusion_matrix(y_val.values, val_predictions))\n",
    "print(classification_report(y_val.values, val_predictions))\n",
    "print(\"Accuracy (validation):\", accuracy_score(y_val.values, val_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "### PIPELINE GRID_SEARCH ###\n",
    "############################\n",
    "\n",
    "# Parameter grid for GridSearch\n",
    "param_grid={'decision_tree__max_depth': [15, 25, 35], #, 45, 55, 75], \n",
    "            'decision_tree__min_samples_leaf': [5, 10, 15], #, 15, 30],\n",
    "           }\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, # Base model\n",
    "                           param_grid, # Parameters to try\n",
    "                           cv = 5, # Apply 5-fold cross validation\n",
    "                           verbose = 1, # Print summary\n",
    "                           n_jobs = -1 # Use all available processors\n",
    "                          )\n",
    "\n",
    "# Fit the GridSearch to our training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Best score: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[866 404]\n",
      " [327 903]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.68      0.70      1270\n",
      "           1       0.69      0.73      0.71      1230\n",
      "\n",
      "    accuracy                           0.71      2500\n",
      "   macro avg       0.71      0.71      0.71      2500\n",
      "weighted avg       0.71      0.71      0.71      2500\n",
      "\n",
      "Accuracy (validation): 0.7076\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('data_preprocessing', data_preprocessor),\n",
    "    ('random_forest_classifier', RandomForestClassifier(n_estimators=150,\n",
    "                                             max_depth = 10,\n",
    "                                             min_samples_leaf = 15))\n",
    "                    ])\n",
    "\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "pipeline\n",
    "\n",
    "pipeline.fit(X_train, y_train.values)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "# Use the fitted pipeline to make predictions on the validation dataset\n",
    "val_predictions = pipeline.predict(X_val)\n",
    "print(confusion_matrix(y_val.values, val_predictions))\n",
    "print(classification_report(y_val.values, val_predictions))\n",
    "print(\"Accuracy (validation):\", accuracy_score(y_val.values, val_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_df = pd.read_csv('../data/imdb_test.csv', header=0)\n",
    "test_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
